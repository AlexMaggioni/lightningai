{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.metrics import f1_score   \n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from transformer import Transformer, MultiHeadedAttention\n",
    "from lstm import EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the assignment folder to Python path\n",
    "if '/content/assignment' not in sys.path:\n",
    "  sys.path.insert(0, '/content/assignment')\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "  warnings.warn('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"yelp_polarity\", split=\"train\", cache_dir=\"assignment/data\")\n",
    "dataset_test = load_dataset(\"yelp_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Truncate/Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"text\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        \n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"bert-base-uncased\"\n",
    "sample_max_length = 256\n",
    "collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone_hidden_size = backbone_hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = AutoModel.from_pretrained(\n",
    "            self.backbone,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = back_bone_output[0]\n",
    "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "class ReviewClassifierLSTM(nn.Module):\n",
    "    def __init__(self, nb_classes: int, encoder_only: bool = False,\n",
    "        with_attn: bool = True, dropout: int = 0.5, hidden_size: int = 256):\n",
    "        super(ReviewClassifierLSTM, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.encoder_only = encoder_only\n",
    "\n",
    "        if with_attn:\n",
    "            attn = MultiHeadedAttention(head_size = 2*hidden_size, num_heads=1)\n",
    "        else:\n",
    "            attn = None\n",
    "\n",
    "        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only,\n",
    "                                        attn=attn, hidden_size=hidden_size)\n",
    "\n",
    "        if self.encoder_only:\n",
    "            self.classifier = torch.nn.Linear(hidden_size*2, self.nb_classes)\n",
    "        else:\n",
    "            self.classifier = torch.nn.Linear(hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        pooled_output, _ = self.back_bone(input_ids, attention_mask)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ReviewClassifierTransformer(nn.Module):\n",
    "    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str=\"prenorm\", dropout: float = 0.3):\n",
    "        super(ReviewClassifierTransformer, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n",
    "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        attention_mask = torch.cat([torch.ones(attention_mask.shape[0]).unsqueeze(1).to(device),\n",
    "                                    attention_mask], dim=1)\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask)\n",
    "        hidden_states = back_bone_output\n",
    "        pooled_output = hidden_states\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Device selected: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"--> Device selected: {device}\")\n",
    "def train_one_epoch(\n",
    "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0\n",
    "    logging_loss = 0\n",
    "    start_time = time.time()\n",
    "    mini_start_time = time.time()\n",
    "    for step, batch in enumerate(training_data_loader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        logging_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % logging_frequency == 0:\n",
    "            freq_time = time.time()-mini_start_time\n",
    "            logger['train_time'].append(freq_time+logger['train_time'][-1])\n",
    "            logger['train_losses'].append(logging_loss/logging_frequency)\n",
    "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
    "            eval_acc, eval_f1, eval_loss, eval_time = evaluate(model, testing_data_loader)\n",
    "            logger['eval_accs'].append(eval_acc)\n",
    "            logger['eval_f1s'].append(eval_f1)\n",
    "            logger['eval_losses'].append(eval_loss)\n",
    "            logger['eval_time'].append(eval_time+logger['eval_time'][-1])\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                current_memory_usage = torch.cuda.memory_allocated(device)/1024**3\n",
    "                logger['memory_usage'].append(current_memory_usage)\n",
    "\n",
    "            logging_loss = 0\n",
    "            mini_start_time = time.time()\n",
    "\n",
    "    return epoch_loss / len(training_data_loader), time.time()-start_time\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    eval_loss = 0\n",
    "    correct_predictions = {i: 0 for i in range(2)}\n",
    "    total_predictions = {i: 0 for i in range(2)}\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_data_loader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
    "            preds.extend(predictions.tolist())\n",
    "            targets.extend(batch[\"labels\"].cpu().numpy().tolist())\n",
    "\n",
    "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
    "                if target == prediction:\n",
    "                    correct_predictions[target] += 1\n",
    "                total_predictions[target] += 1\n",
    "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
    "    f1 = f1_score(targets, preds)\n",
    "    model.train()\n",
    "    return accuracy, round(f1, 4), eval_loss / len(test_data_loader), time.time() - start_time\n",
    "\n",
    "\n",
    "def save_logs(dictionary, log_dir, exp_id):\n",
    "  log_dir = os.path.join(log_dir, exp_id)\n",
    "  os.makedirs(log_dir, exist_ok=True)\n",
    "  # Log arguments\n",
    "  with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
    "    json.dump(dictionary, f, indent=2)\n",
    "\n",
    "def save_model(model, log_dir, exp_id):\n",
    "  log_dir = os.path.join(log_dir, exp_id)\n",
    "  os.makedirs(log_dir, exist_ok=True)\n",
    "  # Save model\n",
    "  torch.save(model.state_dict(), os.path.join(log_dir, f\"model_{exp_id}.pt\"))\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 5: Transformer, 2 layers, pre-normalization\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mencoder.layer.11\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m         param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m train_loss, train_time \u001b[39m=\u001b[39m train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m eval_acc, eval_f1, eval_loss, eval_time  \u001b[39m=\u001b[39m evaluate(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m logger[\u001b[39m\"\u001b[39m\u001b[39mepoch_train_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m logging_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hrxa33tc8g5pd5qs5xmca49m.studio.lightning.ai/teamspace/studios/this_studio/IFT6135_HW2/src/experiment.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m logging_frequency \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging_frequency = 100\n",
    "learning_rate = 1e-5\n",
    "nb_epoch=5\n",
    "\n",
    "for i in range(5, 9):\n",
    "  experimental_setting = i\n",
    "\n",
    "  if experimental_setting == 1:\n",
    "    print(\"Setting 1: LSTM, no dropout, encoder only\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0, encoder_only=True)\n",
    "  if experimental_setting == 2:\n",
    "    print(\"Setting 2: LSTM, dropout, encoder only\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n",
    "  if experimental_setting == 3:\n",
    "    print(\"Setting 3: LSTM, dropout, encoder-decoder, no attention\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=False)\n",
    "  if experimental_setting == 4:\n",
    "    print(\"Setting 4: LSTM, dropout, encoder-decoder, with attention\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=True)\n",
    "  if experimental_setting == 5:\n",
    "    print(\"Setting 5: Transformer, 2 layers, pre-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n",
    "  if experimental_setting == 6:\n",
    "    print(\"Setting 6: Transformer, 4 layers, pre-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n",
    "  if experimental_setting == 7:\n",
    "    print(\"Setting 7: Transformer, 2 layers, post-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n",
    "  if experimental_setting == 8:\n",
    "    nb_epoch = 2\n",
    "    print(\"Setting 8: Fine-tuning BERT\")\n",
    "    model = ReviewClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n",
    "    for parameter in model.back_bone.parameters():\n",
    "      parameter.requires_grad= False\n",
    "\n",
    "\n",
    "  # setting up the optimizer\n",
    "  optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n",
    "  model.to(device)\n",
    "  logger = {\n",
    "      'train_time': [0],  # Training time starts with 0 for cumulative tracking\n",
    "      'eval_time': [0],   # Evaluation time starts with 0 for cumulative tracking\n",
    "      'train_losses': [],\n",
    "      'eval_accs': [],\n",
    "      'eval_f1s': [],\n",
    "      'eval_losses': [],\n",
    "      'memory_usage': [],  # Add this line for average memory usage logging\n",
    "      'epoch_train_loss': [],\n",
    "      'epoch_train_time': [],\n",
    "      'epoch_eval_loss': [],\n",
    "      'epoch_eval_time': [],\n",
    "      'epoch_eval_acc': [],\n",
    "      'epoch_eval_f1': [],\n",
    "      'parameters': sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "  }\n",
    "\n",
    "  for epoch in range(nb_epoch):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    if experimental_setting == 8 and epoch>1: #unfreezing layer 10 for fine-tuning\n",
    "      for name, param in model.back_bone.named_parameters():\n",
    "        if name.startswith(\"encoder.layer.11\"):\n",
    "            param.requires_grad = True\n",
    "    train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n",
    "    eval_acc, eval_f1, eval_loss, eval_time  = evaluate(model, test_loader)\n",
    "    logger[\"epoch_train_loss\"].append(train_loss)\n",
    "    logger[\"epoch_train_time\"].append(train_time)\n",
    "    logger[\"epoch_eval_loss\"].append(eval_loss)\n",
    "    logger[\"epoch_eval_time\"].append(eval_time)\n",
    "    logger[\"epoch_eval_acc\"].append(eval_acc)\n",
    "    logger[\"epoch_eval_f1\"].append(eval_f1)\n",
    "    print(f\"    Epoch: {epoch+1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, F1/Test: {eval_f1}, Train Time: {train_time}, Eval Time: {eval_time}\")\n",
    "\n",
    "  logger['train_time'] = logger['train_time'][1:]\n",
    "  logger['eval_time'] = logger['eval_time'][1:]\n",
    "  save_logs(logger, \"assignment/log\", str(experimental_setting))\n",
    "  save_model(model, \"assignment/models\", str(experimental_setting))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
